
# **INFORME DE INVESTIGACION**
![Enlace a GitHub](Assets/ups-icc.png)


### **Asignatura:** Estructura de Datos

### **Tema:** Proyecto de Complejidad y Eficiencia de Algoritmos


# Integrantes:
- Axel Banegas - [Enlace a GitHub](https://github.com/axelbanegas)
- Pedro Panjon - [Enlace a GitHub](https://github.com/PedroP57xd)

# Objetivos:

- Adquirir mayor conocimiento sobre la eficiencia de los distintos algoritmos.

- Poder reconocer la complejidad de los algoritmos implementados dentro de un programa.


# Marco Te√≥rico: 

## **1. Teor√≠a de la Complejidad**

*(Aqu√≠ el estudiante coloca su investigaci√≥n)*

### 1.1 Definici√≥n general
Es un enfoque cient√≠fico que estudia sistemas complejos (como los biol√≥gicos, sociales o naturales) reconociendo que no se pueden entender analizando sus partes de forma aislada, sino como un todo interconectado.

### 1.2 Importancia en la resoluci√≥n de problemas

La teor√≠a de la complejidad computacional es fundamental para entender qu√© tan dif√≠cil es resolver un problema con un algoritmo. No solo clasifica los problemas seg√∫n los recursos necesarios (tiempo y espacio), sino que tambi√©n gu√≠a la elecci√≥n y el dise√±o de soluciones eficientes.

  üîç  ¬øPor qu√© es importante?

- Permite evaluar la eficiencia de los algoritmos: Saber si un algoritmo es O(n), O(n¬≤) u O(2‚Åø) ayuda a anticipar su rendimiento a diferentes escalas.

-    Ayuda a identificar problemas intratables: Algunos problemas, como los NP-completos, no tienen soluciones eficientes conocidas. Reconocerlos permite evitar enfoques inviables y buscar aproximaciones o heur√≠sticas.

 -   Optimiza el uso de recursos: Un buen entendimiento de la complejidad evita desperdiciar tiempo computacional y memoria.

  -  Gu√≠a la toma de decisiones: En proyectos reales, un algoritmo ‚Äúperfecto‚Äù pero demasiado lento puede ser in√∫til. Analizar la complejidad ayuda a elegir la estrategia m√°s pr√°ctica.

   - Fomenta mejores pr√°cticas de desarrollo: Comprender el impacto del crecimiento de los datos obliga a escribir c√≥digo m√°s limpio, escalable y sostenible.

### 1.3 Eficiencia de algoritmos

* Coste temporal:

    El coste temporal de un algoritmo se refiere al n√∫mero de operaciones que debe realizar para completar su tarea en funci√≥n del tama√±o de la entrada n. Se mide con notaci√≥n Big-O, que describe c√≥mo crece el tiempo de ejecuci√≥n cuando aumentan los datos. Por ejemplo, una b√∫squeda lineal tiene un coste O(n) porque necesita recorrer todos los elementos, mientras que una b√∫squeda binaria reduce el tiempo a O(\log n) al dividir el problema en mitades sucesivas.

* Coste espacial:

    El coste espacial mide la cantidad de memoria que un algoritmo necesita para ejecutarse correctamente. Incluye la memoria fija (variables y c√≥digo), la memoria dependiente del tama√±o de la entrada y la memoria adicional usada en estructuras auxiliares o recursi√≥n. Por ejemplo, MergeSort requiere O(n) espacio extra para arreglos temporales, mientras que algoritmos in-place como Quicksort solo necesitan O(\log n) espacio por la pila de recursi√≥n.


### 1.4 Factores de tiempo de ejecuci√≥n

* Factores propios:

    Son caracter√≠sticas internas del algoritmo que determinan su eficiencia, independientemente del entorno donde se ejecute.

    Estructura del algoritmo: n√∫mero de bucles, recursi√≥n, llamadas anidadas, etc.

    Complejidad temporal y espacial: qu√© tan r√°pido crecen las operaciones respecto al tama√±o de entrada.

    Eficiencia de las operaciones usadas: b√∫squedas, ordenamientos, acceso a estructuras de datos, etc.

    Elecci√≥n de estructuras de datos: algunas permiten operaciones m√°s r√°pidas (por ejemplo, hash tables vs. listas).

    Estos factores no cambian seg√∫n el hardware: dependen solo del dise√±o del algoritmo.

* Factores circunstanciales:

    Son condiciones externas que pueden afectar el tiempo real que tarda un algoritmo en ejecutarse.

    - Hardware disponible: velocidad del procesador, n√∫mero de n√∫cleos, memoria RAM, disco, GPU.

    - Carga del sistema: otros procesos ejecut√°ndose simult√°neamente.

    - Lenguaje de programaci√≥n y compilador: algunos lenguajes generan c√≥digo m√°s optimizado.

    - Implementaci√≥n espec√≠fica de librer√≠as o funciones est√°ndar.

    - Sistema operativo y arquitectura.

    - Estos factores pueden variar incluso con el mismo algoritmo y afectan el rendimiento pr√°ctico.

üìä M√©todos de an√°lisis del rendimiento 
    
* An√°lisis te√≥rico:

    Estudia el rendimiento del algoritmo de forma abstracta, independientemente del hardware.

    - Utiliza notaciones como O(n), Œ©(n), Œò(n).

    - Permite comparar algoritmos seg√∫n su crecimiento asint√≥tico.

    - No mide tiempos reales, sino c√≥mo escalan las operaciones para entradas grandes.

    - Es √∫til para elegir el algoritmo m√°s adecuado desde una perspectiva general.

    Ejemplo: determinar que un algoritmo de ordenamiento es O(n log n) sin ejecutarlo.

* An√°lisis experimental

    Eval√∫a el comportamiento real del algoritmo mediante pruebas pr√°cticas.

    - Se mide el tiempo de ejecuci√≥n en distintos escenarios y tama√±os de entrada.

    - Permite observar el impacto de factores circunstanciales.

    - √ötil para comparar implementaciones, lenguajes, m√°quinas o versiones de c√≥digo.

    - Puede descubrir comportamientos inesperados no visibles en el an√°lisis te√≥rico.

    Ejemplo: medir cu√°nto tarda realmente un algoritmo para n = 10‚Å∂ en una m√°quina espec√≠fica.

### 1.5 Notaci√≥n de complejidad

* Big O:

    La notaci√≥n Big O describe el l√≠mite superior del crecimiento de un algoritmo, es decir, c√≥mo aumenta el n√∫mero de operaciones en funci√≥n del tama√±o de la entrada n. Se utiliza para expresar el peor comportamiento posible en t√©rminos de tiempo o espacio, ignorando constantes y factores menores. Por ejemplo, un algoritmo de ordenamiento r√°pido tiene complejidad O(n\log n) en promedio, lo que significa que su tiempo de ejecuci√≥n crece proporcionalmente a n\log n.

* Mejor caso:

     El mejor caso representa la situaci√≥n m√°s favorable para un algoritmo, cuando la entrada est√° en condiciones √≥ptimas y el n√∫mero de operaciones es m√≠nimo. Por ejemplo, en la b√∫squeda lineal, si el elemento buscado est√° en la primera posici√≥n, el coste es O(1). Aunque es √∫til conocerlo, rara vez se usa como referencia principal porque no refleja el comportamiento t√≠pico.
* Peor caso:

    El peor caso indica el m√°ximo n√∫mero de operaciones que un algoritmo puede necesitar, es decir, el escenario m√°s desfavorable. Es el m√°s utilizado en an√°lisis porque garantiza que el algoritmo no ser√° m√°s lento que ese l√≠mite. Por ejemplo, en la b√∫squeda lineal, si el elemento est√° al final o no existe, el coste es O(n).

* Caso promedio:

    El caso promedio estima el n√∫mero esperado de operaciones considerando todas las posibles entradas y su probabilidad de ocurrencia. Es m√°s realista que el mejor caso y m√°s optimista que el peor caso. Por ejemplo, en la b√∫squeda lineal, si el elemento est√° distribuido aleatoriamente, el coste esperado es , que se simplifica a .

* Big O, Œ©, Œò:

    Estas tres notaciones describen distintos l√≠mites de complejidad. Big O () indica el l√≠mite superior (m√°ximo crecimiento). Omega () indica el l√≠mite inferior (m√≠nimo crecimiento posible). Theta () indica el crecimiento exacto cuando el l√≠mite superior e inferior coinciden. Por ejemplo, para la b√∫squeda lineal, el tiempo de ejecuci√≥n es  en el mejor caso,  en el peor caso, y  en el comportamiento general.


---

## **2. Ejemplos de Complejidad en Java**

---

## **2.1 Complejidad O(1) ‚Äì Constante**

### **Archivo:** `ComplejidadConstante.java`

### **C√≥digo del ejemplo**

```java
public void ejemplo() {
    System.out.println("Ejemplo O(1)");
    int x = 10;
    int y = 5;
    int suma = x + y;
}
```

### **Explicaci√≥n resumida**

Este algoritmo siempre realiza la misma cantidad de operaciones sin importar el tama√±o de la entrada. La suma de dos n√∫meros y la impresi√≥n en pantalla son acciones fijas, por lo que el tiempo de ejecuci√≥n no crece con n.

---
## **2.2 Complejidad O(n) ‚Äì Tiempo lineal**

### **Archivo:** `ComplejidadLineal.java`

### **C√≥digo del ejemplo**

```java
public static void lineal(int[] arr) {
        for (int num : arr) {
            System.out.println(num);
        }
    }
```

### **Explicaci√≥n resumida**

Explicaci√≥n resumida: El algoritmo recorre todos los elementos del arreglo una sola vez. El n√∫mero de operaciones depende directamente del tama√±o de la entrada n. Si el arreglo tiene 10 elementos, se hacen 10 impresiones; si tiene 100, se hacen 100. Por eso es lineal.

---
## **2.3 Complejidad O(n¬≤) ‚Äì Tiempo cuadr√°tico**

### **Archivo:** `ComplejidadCuadratica.java`

### **C√≥digo del ejemplo**

```java
public static void cuadratica(int[] arr) {
        for (int i : arr) {
            for (int j : arr) {
                System.out.println(i + ", " + j);
            }
        }
    }
```

### **Explicaci√≥n resumida**

Explicaci√≥n resumida: Aqu√≠ se usan dos bucles anidados, lo que significa que por cada elemento del arreglo se recorre nuevamente todo el arreglo. El n√∫mero de operaciones es proporcional a n\times n, es decir, n^2. Por eso se clasifica como cuadr√°tica.

---
## **2.4 Complejidad O(log n) ‚Äì Tiempo logar√≠tmico**

### **Archivo:** `ComplejidadLogaritmico.java`

### **C√≥digo del ejemplo**

```java
public static boolean logaritmica(int[] arr, int objetivo) {
        int inicio = 0;
        int fin = arr.length - 1;

        while (inicio <= fin) {
            int medio = (inicio + fin) / 2;

            if (arr[medio] == objetivo) {
                return true;
            } else if (arr[medio] < objetivo) {
                inicio = medio + 1;
            } else {
                fin = medio - 1;
            }
        }
        return false;
    }
```

### **Explicaci√≥n resumida**

Explicaci√≥n resumida: El algoritmo aplica b√∫squeda binaria, dividiendo el arreglo a la mitad en cada iteraci√≥n. En lugar de recorrer todos los elementos, reduce el problema a la mitad sucesivamente. El n√∫mero de pasos crece en proporci√≥n a \log n, lo que lo hace logar√≠tmico.

---
## **2.5 Complejidad O(n log n) ‚Äì Merge Sort**

### **Archivo:** `ComplejidadMergeSort.java`

### **C√≥digo del ejemplo**

```java
public static int[] mergeSort(int[] arr) {
        if (arr.length <= 1) {
            return arr;
        }

        int mid = arr.length / 2;
        int[] izquierda = java.util.Arrays.copyOfRange(arr, 0, mid);
        int[] derecha = java.util.Arrays.copyOfRange(arr, mid, arr.length);

        return merge(mergeSort(izquierda), mergeSort(derecha));
    }
```

### **Explicaci√≥n resumida**

Explicaci√≥n resumida: Merge Sort divide el arreglo en mitades recursivamente (\log n divisiones) y en cada nivel combina los elementos (n operaciones). Al multiplicar ambos factores, la complejidad total es O(n\log n).

---


# **Conclusiones**

1. En el presente informe adquirimos diferentes conocimientos acerca de los algoritmos de ordenamiento y su complejidad dentro de un programa, cu√°les son sus caracter√≠sticas y factores que influyen en el desarrollo de los mismos y c√≥mo podemos aprovechar sus recursos para hacerlos una herramienta en nuestros futuros programas. 

**POR ESTUDIANTE**: Axel David Banegas Lazo

2. En conclusion, gracias a esta investigacion se logro comprender lo que necesita un programa tanto en tiempo y espacio para poder funcionar de la manera mas eficiente, y mas especificamente en los metodos que se han venido trabajando en las ultimas clases, asi podremos comparar varios algoritmos y poder eleguir correctamente el mas eficiente para cada problematica, creando asi codigo rapido y ordenado. 

**POR ESTUDIANTE**: Pedro Fernando Panj√≥n Peralta

---